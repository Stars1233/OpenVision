<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OpenVision: Project Page</title>
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./resources/icon.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
</head>
<body>

<!-- Hero -->
<section class="hero is-primary">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h1 class="title is-1">OpenVision</h1>
      <h2 class="subtitle is-4">Fully-Open, Cost-Effective Vision Encoders for Multimodal Learning</h2>
      <p style="font-size: 1.15em; font-weight: 500; margin-top: 1.2em;">
        Xianhang Li · Yanqing Liu · Haoqin Tu · Hongru Zhu · Cihang Xie
      </p>
      <p style="font-size: 1.05em; color: rgba(255, 255, 255, 0.9);">
        University of California, Santa Cruz
      </p>

      <!-- Buttons -->
      <div class="publication-links" style="margin-top: 1.5em;">
        <!-- PDF Link -->
        <span class="link-block">
          <a href="https://arxiv.org/abs/2406.08478"
             class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <img src="./resources/ar.svg" alt="arXiv" style="width: 1.2em; height: 1.2em;" />
            </span>
            <span>arXiv</span>
          </a>
        </span>

        <!-- Code Link -->
        <span class="link-block">
          <a href="https://github.com/UCSC-VLAA/OpenVision"
             class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fab fa-github"></i>
            </span>
            <span>Code</span>
          </a>
        </span>

        <!-- Model Link -->
        <span class="link-block">
          <a href="https://huggingface.co/datasets/UCSC-VLAA/OpenVision"
             class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <img src="./resources/gr.svg" alt="HF" style="width: 1.2em; height: 1.2em;" />
            </span>
            <span>Model</span>
          </a>
        </span>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<div style="max-width: 900px; margin: 2em auto; text-align: left;">
  <h2 class="title is-3 has-text-centered">Abstract</h2>
  <p style="font-size: 1.1em; line-height: 1.6;">
    OpenAI's CLIP, released in early 2021, have long been the go-to choice of vision encoder for building multimodal foundation models. Although recent alternatives such as SigLIP have begun to challenge this status quo, to our knowledge none are fully open: their training data remains proprietary and/or their training recipes are not released.
This paper fills this gap with OpenVision, a fully-open, cost-effective family of vision encoders that match or surpass the performance of OpenAI’s CLIP when integrated into multimodal frameworks like LLaVA. OpenVision builds on existing works---e.g., CLIPS for training framework and Recap-DataComp-1B for training data---while revealing multiple key insights in enhancing encoder quality and showcasing practical benefits in advancing multimodal models.  By releasing vision encoders spanning from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible trade-off between capacity and efficiency in building multimodal models: larger models deliver enhanced multimodal performance, while smaller versions enable lightweight, edge-ready multimodal deployments.
  </p>
</div>


<!-- Key Contributions -->
<section class="section has-background-light">
  <div class="container">
    <h2 class="title is-3">Key Contributions</h2>
    <div class="content">
      <ul>
        <li>Completely open training recipe: datasets, codebase, and checkpoints are public.</li>
        <li>Family of ViT encoders spanning Tiny (5.9M) to Huge (632.1M) parameters.</li>
        <li>Superior multimodal performance under LLaVA-1.5 and Open-LLaVA-Next benchmarks.</li>
        <li>Efficient progressive resolution training: 2×–3× faster than proprietary counterparts.</li>
        <li>Supports flexible patch sizes (8×8, 16×16) for detailed or efficient encoding.</li>
      </ul>
    </div>
  </div>
</section>

<!-- Detailed Comparisons Section -->
<section class="section has-background-light">
  <div class="container has-text-centered">
    <h2 class="title is-3">Detailed Comparisons and Efficiency</h2>

    <!-- OpenVision vs Proprietary -->
    <div class="content" style="margin-bottom: 2em;">
      <h3 class="title is-4">OpenVision vs. Proprietary Encoders</h3>
      <img src="resources/openvision_teaser_v1.3.png" alt="OpenVision vs Proprietary Encoders"
           style="max-width: 65%; height: auto; border: 1px solid #ccc; box-shadow: 0 2px 6px rgba(0,0,0,0.1);">
      <p style="margin-top: 0.8em;">
        OpenVision encoders match or outperform proprietary models like OpenAI's CLIP and Google's SigLIP across multimodal tasks.
      </p>
    </div>

    <!-- LLaVA-1.5 Performance -->
    <div class="content" style="margin-bottom: 2em;">
      <h3 class="title is-4">Performance under LLaVA-1.5 Framework</h3>
      <img src="resources/performance_normalized.png" alt="LLaVA-1.5 Performance Comparison"
           style="max-width: 70%; height: auto; border: 1px solid #ccc; box-shadow: 0 2px 6px rgba(0,0,0,0.1);">
      <p style="margin-top: 0.8em;">
        OpenVision demonstrates strong performance improvements over existing CLIP models under the LLaVA-1.5 multimodal framework.
      </p>
    </div>

    <!-- Open-LLaVA-Next Performance -->
    <div class="content" style="margin-bottom: 2em;">
      <h3 class="title is-4">Performance under Open-LLaVA-Next Framework</h3>
      <img src="resources/performance_normalized_2.png" alt="Open-LLaVA-Next Performance Comparison"
           style="max-width: 70%; height: auto; border: 1px solid #ccc; box-shadow: 0 2px 6px rgba(0,0,0,0.1);">
      <p style="margin-top: 0.8em;">
        Under Open-LLaVA-Next, OpenVision maintains its competitive edge, excelling particularly in document-heavy multimodal tasks.
      </p>
    </div>

    <!-- Efficiency Comparison -->
    <div class="content">
      <h3 class="title is-4">Efficiency Comparison</h3>
      <img src="resources/efficiency_llava1.5_vs_next.png" alt="Efficiency Comparison"
           style="max-width: 70%; height: auto; border: 1px solid #ccc; box-shadow: 0 2px 6px rgba(0,0,0,0.1);">
      <p style="margin-top: 0.8em;">
        OpenVision achieves superior multimodal performance with significantly reduced training time compared to proprietary alternatives.
      </p>
    </div>

  </div>
</section>


<!-- Model Variants -->
<section class="section has-background-light">
  <div class="container">
    <h2 class="title is-3">Model Variants</h2>
    <table class="table is-fullwidth is-striped">
      <thead>
        <tr>
          <th>Variant</th><th># Params</th><th>Patch Size</th><th>Resolution Stages</th>
        </tr>
      </thead>
      <tbody>
        <tr><td>Tiny (Ti)</td><td>5.9M</td><td>8×8 / 16×16</td><td>160→224→384</td></tr>
        <tr><td>Small (S)</td><td>22.4M</td><td>8×8 / 16×16</td><td>160→224→384</td></tr>
        <tr><td>Base (B)</td><td>87.4M</td><td>8×8 / 16×16</td><td>84→224→336/384</td></tr>
        <tr><td>Large (L)</td><td>303.7M</td><td>14×14</td><td>84→224→336/384</td></tr>
        <tr><td>SoViT-400M</td><td>400M</td><td>14×14</td><td>84→224→384</td></tr>
        <tr><td>Huge (H)</td><td>632.1M</td><td>14×14</td><td>84→224→336</td></tr>
      </tbody>
    </table>
  </div>
</section>

<!-- Get Started -->
<section class="section">
  <div class="container">
    <h2 class="title is-3">Get Started</h2>
    <div class="content">
      <p>Install and load a pre-trained model with Hugging Face:</p>
      <pre><code>pip install transformers timm

from transformers import CLIPProcessor, CLIPModel
model = CLIPModel.from_pretrained('UCSC-VLAA/OpenVision-B-16')
processor = CLIPProcessor.from_pretrained('UCSC-VLAA/OpenVision-B-16')</code></pre>
    </div>
  </div>
</section>

<!-- Footer -->
<footer class="footer">
  <div class="content has-text-centered">
    <p>Built by the UCSC-VLAA team. <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a>,
    <a href="https://yanqing0327.github.io/Yanqing.github.io/">Yanqing Liu</a>, Haoqin Tu, Hongru Zhu, Cihang Xie.</p>
  </div>
</footer>

</body>
</html>
